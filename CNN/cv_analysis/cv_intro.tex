\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{siunitx}
\geometry{margin=1in}

\title{Cross-Validation Based CNN for Multi-Step Household Energy Forecasting}
\author{ECS171G13 Team \\ (Cross-Validation Demo Owner: YOUR NAME)}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}
Multi-step time-series forecasting models often risk overfitting to a specific temporal segment when trained with a single static train/validation split. To mitigate this, we adopt \emph{time-aware cross-validation} using a forward-expanding (TimeSeriesSplit) strategy. This approach provides:
\begin{itemize}
  \item More robust generalization estimates across different historical ranges.
  \item Reduced sensitivity to idiosyncratic patterns in any one validation slice.
  \item A principled selection of hyperparameters or final model checkpoint.
\end{itemize}

\section{Task Definition and Data Windowing}
We reframe the household energy dataset into supervised windows:
\[
  X^{(i)} \in \mathbb{R}^{T_{\text{in}} \times C}, \quad y^{(i)} \in \mathbb{R}^{T_{\text{out}}},
\]
where $T_{\text{in}} = 72$ (past 72 hours), $T_{\text{out}} = 24$ (future 24 hours), and $C=4$ channels correspond to sub-metering measurements:
\[
  \text{Channels} = \{ \text{Sub\_metering\_1}, \text{Sub\_metering\_2}, \text{Sub\_metering\_3}, \text{Sub\_metering\_rest} \}.
\]
Data preparation (via \texttt{DataProcessor}) performs:
\begin{enumerate}
  \item Cleaning and resampling raw consumption data.
  \item Scaling and aligning timestamps.
  \item Generating rolling window pairs $(X^{(i)}, y^{(i)})$ with stride 1.
\end{enumerate}

\section{Model Architecture}
We implement a 1D convolutional neural network (CNN) for sequence feature extraction followed by fully-connected layers producing a 24-hour horizon forecast:
\begin{align*}
\text{Input shape: } & (B, T_{\text{in}}, C) \rightarrow \text{internally permuted to } (B, C, T_{\text{in}})\\
\text{Conv blocks: } & [32, 64, 128] \text{ channels},~\text{kernel size }3,~\text{max-pool }2\\
\text{Dense layers: } & \text{Flatten} \rightarrow 256 \text{ hidden units} \rightarrow 24 \text{ outputs}\\
\end{align*}
Configured hyperparameters:
\begin{verbatim}
in_channels = 4
input_length = 24 * 3  (72)
output_steps = 24
conv_channels = [32, 64, 128]
kernel_size = 3
pool_kernel = 2
fc_hidden = 256
\end{verbatim}

\section{Training Procedure}
For cross-validation:
\begin{itemize}
  \item Build $K$ folds using a time-series split (no leakage from future to past).
  \item For each fold $k$, train on earlier windows and validate on the next chronological segment.
  \item Record per-epoch train and validation MSE.
  \item Select the model checkpoint with lowest final validation loss across folds.
\end{itemize}

We also train a baseline model using the original single train/validation partition for comparison.

\subsection{Loss Curves}
Figure~\ref{fig:cv_loss_curves} shows per-fold training and validation curves, indicating convergence stability; Figure~\ref{fig:cv_final_val_losses} compares final validation losses across folds.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{CNN/cv_analysis/figs/cv_loss_curves.png}
  \caption{Per-fold train/validation MSE curves.}
  \label{fig:cv_loss_curves}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\textwidth]{CNN/cv_analysis/figs/cv_final_val_losses.png}
  \caption{Final validation MSE per fold.}
  \label{fig:cv_final_val_losses}
\end{figure}

\subsection{Checkpointing}
During training, the best validation loss model per fold is saved:
\[
\texttt{cv\_analysis/cv\_ckpts/fold\_k/foldk\_best.pt}.
\]
The best overall fold model is then evaluated on the test set.

\section{Results and Analysis}
We evaluate both the CV-selected model and the baseline on:
\begin{enumerate}
  \item Overall horizon MSE: $\\text{MSE}_{\\text{all}} = \\frac{1}{N T_{out}} \\sum_{i,t} (\\hat{y}_{i,t}-y_{i,t})^2$.
  \item Per-step horizon MSE vector.
  \item Daily sum MSE: aggregate predicted vs actual total consumption over 24h.
\end{enumerate}

Illustrative forecast samples (Figures~\ref{fig:cv_sample_forecast}, \ref{fig:baseline_sample_forecast}) highlight prediction alignment quality.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\\textwidth]{CNN/cv_analysis/figs/sample_0_forecast.png}
  \caption{CV-selected model forecast (sample index 0).}
  \label{fig:cv_sample_forecast}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\\textwidth]{CNN/cv_analysis/figs/sample_1_forecast.png}
  \caption{Baseline model forecast (sample index 1).}
  \label{fig:baseline_sample_forecast}
\end{figure}

% Placeholder table - replace values after running analysis.py
\\begin{table}[h]
\\centering
\\begin{tabular}{lrr}
\\toprule
Metric & CV Model & Baseline \\\\ \\midrule
Final Val Loss (best fold) & X.XXX & Y.YYY \\\\
Test Overall MSE & A.AAA & B.BBB \\\\
Daily Sum MSE & C.CCC & D.DDD \\\\
\\bottomrule
\\end{tabular}
\\caption{Comparison between cross-validation selected model and single-split baseline. Replace placeholders with actual numbers from metrics.json.}
\\end{table}

\\subsection{Discussion}
Cross-validation yields a more reliable estimate of generalization, avoiding optimistic or pessimistic bias from a single split. Variability across folds informs stability; if fold losses are consistent, the model demonstrates robustness. Any substantial gap between baseline and CV performance suggests overfitting avoidance via broader temporal coverage.

\\section{Conclusion}
Employing time-aware cross-validation with a CNN architecture for 72h\\to24h forecasting improves reliability of performance estimates and enables better checkpoint selection. Future work may incorporate:
\\begin{itemize}
  \\item Attention or temporal fusion transformers for richer temporal dependencies.
  \\item Probabilistic forecasting (prediction intervals).
  \\item Feature augmentation (weather, calendar effects).
\\end{itemize}

\\appendix
\\section{Core Python Snippets}
Listing~\\ref{lst:cv_core} shows the essential CV invocation; Listing~\\ref{lst:analysis_script} provides the main analysis script used to generate figures and metrics.

\\begin{verbatim}
# Cross-validation invocation (simplified)
histories, val_losses, best_model = cross_validate(
    make_model,
    folds,
    device=None,
    epochs=5,
    batch_size=32,
    lr=1e-3,
    verbose=True,
    checkpoint_dir='CNN/cv_analysis/cv_ckpts',
    save_best_only=True
)
\\end{verbatim}

\\section*{References}
(Optionally include references to time-series CV best practices, e.g. Bergmeir et al. on forecasting evaluation.)
\end{document}