{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b1b40a",
   "metadata": {},
   "source": [
    "#### Test input steps\n",
    "\n",
    "Compute losses and accuracy of the models when providing different input steps ranging from 24 hrs to 7*24 hrs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5658d7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11f752830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from cnn_model_yin import CNN, cross_validate, train\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "# load data through the data preprocessor\n",
    "sys.path.append(os.path.abspath('..'))  # add parent directory to sys.path\n",
    "from data_cleanup import DataProcessor\n",
    "\n",
    "# Reproducibility (best-effort)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd2238",
   "metadata": {},
   "source": [
    "### Experiment No.1\n",
    "Just testing input steps with a fixed kernel size for the convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b50248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------- TESTING INPUT STEPS:  24 ----------------\n",
      "\n",
      "Step 1/5: Fetching, cleaning, and engineering features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipejaracaceres/Documents/UCD/Machine Learning/ECS171G13/venv/lib/python3.11/site-packages/ucimlrepo/fetch.py:97: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_url)\n",
      "/Users/felipejaracaceres/Documents/UCD/Machine Learning/ECS171G13/data_cleanup.py:132: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2/5: Resampling data to hourly and setting 'Global_active_power' as target...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipejaracaceres/Documents/UCD/Machine Learning/ECS171G13/data_cleanup.py:172: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df_hourly = df.resample('H').agg(agg_dict)\n",
      "/Users/felipejaracaceres/Documents/UCD/Machine Learning/ECS171G13/data_cleanup.py:173: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_hourly = df_hourly.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3/5: Splitting data and applying scaler...\n",
      "Step 4/5: Creating time-series windows...\n",
      "Step 5/5: Data processing complete.\n",
      "Number of parameters:  627960\n",
      "Epoch 1/20 - train_loss: 0.016564 - val_loss: 0.014551\n",
      "Epoch 2/20 - train_loss: 0.014239 - val_loss: 0.013645\n",
      "Epoch 3/20 - train_loss: 0.013533 - val_loss: 0.013305\n",
      "Epoch 4/20 - train_loss: 0.013236 - val_loss: 0.012903\n",
      "Epoch 5/20 - train_loss: 0.013005 - val_loss: 0.012889\n",
      "Epoch 6/20 - train_loss: 0.012828 - val_loss: 0.013061\n",
      "Epoch 7/20 - train_loss: 0.012614 - val_loss: 0.013108\n",
      "Epoch 8/20 - train_loss: 0.012425 - val_loss: 0.012864\n",
      "Epoch 9/20 - train_loss: 0.012252 - val_loss: 0.012785\n",
      "Epoch 10/20 - train_loss: 0.012051 - val_loss: 0.013019\n",
      "Epoch 11/20 - train_loss: 0.011744 - val_loss: 0.013102\n",
      "Epoch 12/20 - train_loss: 0.011451 - val_loss: 0.013284\n",
      "Epoch 13/20 - train_loss: 0.011180 - val_loss: 0.013660\n",
      "Epoch 14/20 - train_loss: 0.010902 - val_loss: 0.013866\n"
     ]
    }
   ],
   "source": [
    "# Instantiate DataProcessor for 3 days -> 1 day\n",
    "OUTPUT_STEPS = 24\n",
    "INPUT_STEPS_ARRAY = [24, 48, 72, 96, 120, 144, 168]\n",
    "histories = []\n",
    "\n",
    "for INPUT_STEPS in INPUT_STEPS_ARRAY:\n",
    "\n",
    "    print(\"\\n-------------- TESTING INPUT STEPS: \", INPUT_STEPS, \"----------------\\n\")\n",
    "    processor = DataProcessor(input_steps=INPUT_STEPS, output_steps=OUTPUT_STEPS)\n",
    "    Train, Val, Test = processor.load_and_process_data()\n",
    "\n",
    "    X_train, y_train = Train\n",
    "    X_val, y_val = Val\n",
    "    X_test, y_test = Test\n",
    "\n",
    "    model = CNN(8, INPUT_STEPS, OUTPUT_STEPS, kernel_size=3, pool_kernel=0, padding=False) # input all features, no pooling, no padding\n",
    "\n",
    "    # print model num_params\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of parameters: \", num_params)\n",
    "\n",
    "    histories.append(\n",
    "        train(model, X_train, y_train, X_val, y_val, epochs=20)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4b752",
   "metadata": {},
   "source": [
    "### Experiment No.2\n",
    "\n",
    "Adapt the kernel size to the input steps, so that the model can take advantage of larger temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c528424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DataProcessor for 3 days -> 1 day\n",
    "OUTPUT_STEPS = 24\n",
    "INPUT_STEPS_ARRAY = [24, 48, 72, 96, 120, 144, 168]\n",
    "exp2_histories = []\n",
    "\n",
    "for INPUT_STEPS in INPUT_STEPS_ARRAY:\n",
    "\n",
    "    print(\"\\n-------------- TESTING INPUT STEPS: \", INPUT_STEPS, \"----------------\\n\")\n",
    "    processor = DataProcessor(input_steps=INPUT_STEPS, output_steps=OUTPUT_STEPS)\n",
    "    Train, Val, Test = processor.load_and_process_data()\n",
    "\n",
    "    X_train, y_train = Train\n",
    "    X_val, y_val = Val\n",
    "    X_test, y_test = Test\n",
    "\n",
    "    kernel_size = INPUT_STEPS // 8\n",
    "\n",
    "    model = CNN(8, INPUT_STEPS, OUTPUT_STEPS, kernel_size=kernel_size, pool_kernel=0, padding=False) # input all features, no pooling, no padding\n",
    "\n",
    "    # print model num_params\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of parameters: \", num_params)\n",
    "\n",
    "    histories.append(\n",
    "        train(model, X_train, y_train, X_val, y_val, epochs=20)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
